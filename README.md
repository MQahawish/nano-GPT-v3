# nano-GPT-v3

A specialized implementation of GPT optimized for MIDI music generation with **Mixture of Experts (MoE)** and **Multi-head Linear Attention (MLA)**. Built on top of [nanoGPT](https://github.com/karpathy/nanoGPT) with advanced modifications inspired by the [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) paper.

---

## Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Key Features](#key-features)
  - [1. MIDI Encoding System](#1-midi-encoding-system)
  - [2. Data Augmentation](#2-data-augmentation)
  - [3. Advanced Model Architecture](#3-advanced-model-architecture)
  - [4. Tokenization](#4-tokenization)
  - [5. Training Pipeline](#5-training-pipeline)
  - [6. Generation](#6-generation)
- [Visualizing Results](#visualizing-results)
- [Usage](#usage)
- [Configuration](#configuration)
- [Adaptability](#adaptability)
- [Requirements](#requirements)
- [Credits](#credits)

---

## Overview

**nano-GPT-v3** is a framework for training transformer-based language models tailored for generating MIDI music. Its key features include:

- **Custom MIDI Tokenization:** Converts MIDI files into text encodings.
- **MLA with RoPE:** Implements Multi-head Linear Attention using Rotary Position Embeddings.
- **MoE for Scalability:** Utilizes Mixture of Experts for efficient model scaling.
- **Data Augmentation Pipeline:** Enhances MIDI datasets with varied transformations.
- **MIDI Inference:** Generates and exports MIDI music.


## ⚠️ Important Note

To sample music using the pre-trained model I trained (70M parameters), you need to:

1. Download the model checkpoint from GoogleDrive here:
   [Download Model](https://drive.google.com/file/d/1e5yulPjOSvAA3b0yC6EryZeKU4Cf6yW8/view?usp=sharing)

2. Place the downloaded `checkpoint.pt` file in this directory:
``models/3layers12heads32batch512seq-simple/
``
3. Once the checkpoint is in place, you can generate music samples using:
```bash
python sample.py --num_samples 5
```
I believe that better results can be achieved with a larger, more diverse dataset (with augmentations) and more scaling. I was constrained by compute resources as this is a personal project, so I didn't focus on extensive scaling. Still, the music generated by this model is quite good most of the time!

**You can listen to some of the good samples I generated in the `midi-samples` directory!** 🎵

## Project Structure
```
nano-GPT-v3/
├── MIDIVisualizer.exe     # Utility for visualizing generated MIDI files
├── README.md
│
├── diagrams/              # Architecture diagrams
│   ├── BLOCK.png          # Block architecture
│   ├── MLA.png            # Multi-head Linear Attention
│   ├── MODEL.png          # Overall model architecture
│   ├── MOE.png            # Mixture of Experts
│   ├── train_loss.png     # Training loss visualization
│   └── val_loss.png       # Validation loss visualization
│
├── midi-samples/          # Generated MIDI samples
│   ├── sample_1.mid
│   └── ...
│
├── models/                # Trained models
│   └── 3layers12heads32batch512seq-simple/
│       ├── checkpoint.pt  # Model checkpoint (download from GoogleDrive link above)
│       ├── config.pkl     # Serialized configuration
│       ├── config.txt     # Human-readable configuration
│       ├── eval.log       # Evaluation logs
│       ├── training.log   # Training logs
│       └── tensorboard/   # TensorBoard logs
│
└── nanoGPT-v3/            # Core source code
├── augment_all.py     # MIDI data augmentation script
├── config.py          # Configuration management
├── encode_all.py      # MIDI to text encoding script
├── encoding_decoding.py  # Core encoding/decoding system
├── first_silence_cleaner.py  # Utility to clean MIDI encodings
├── model.py           # Core GPT model implementation
├── plot.py            # Training visualization utilities
├── prepare.py         # Dataset preparation script
├── sample.py          # MIDI generation script
├── simple_tokenizer.py  # Tokenizer for MIDI text representations
├── tensorboard_logger.py # Logging utilities
├── train.py           # Training script
└── tokenizers/        # Tokenizer storage
└── simple_tokenizer.json  # The MIDI tokenizer

```
## Key Features

### 1. MIDI Encoding System

- **Core Conversion:** `encoding_decoding.py`
- **Batch Processing:** `encode_all.py`
- **Silence Cleanup:** `first_silence_cleaner.py`

### 2. Data Augmentation

The `augment_all.py` script provides a robust MIDI augmentation framework with:
- **Pitch Shifts**
- **Velocity Changes**
- **Combined Transformations**

### 3. Advanced Model Architecture

Implemented in `model.py`, the architecture includes:
- **MLA (Multi-head Linear Attention):** Enhanced with Rotary Position Embeddings (RoPE)
- **MoE (Mixture of Experts):**
  - **Shared Experts:** Always active
  - **Routed Experts:** Conditionally active based on input
  - **Load Balancing:** Ensures efficient expert utilization
  - **No Token Dropping**

### 4. Tokenization

Handled by `simple_tokenizer.py`, featuring:
- **Note Tokens:** `n0` to `n127`
- **Velocity Tokens:** `v0` to `v127`
- **Duration Tokens:** `d{value}`
- **Special Tokens:** `START`, `sepxx`

### 5. Training Pipeline

The `train.py` script encompasses:
- **Gradient Accumulation**
- **Mixed Precision Training**
- **TensorBoard Logging**
- **Advanced Learning Rate Scheduling**
- **Checkpoint Management**

### 6. Generation

Utilize `sample.py` to generate music with:
- **Temperature Control**
- **Top-k Sampling**
- **Direct MIDI Export**

---

## Visualizing Results

- **MIDI Visualizer:** Use `MIDIVisualizer.exe` to see the musical patterns and structures.
- **Architecture Diagrams:** Located in the `diagrams/` directory:
  - **BLOCK.png:** Transformer block architecture
  - **MLA.png:** Multi-head Linear Attention design
  - **MODEL.png:** Overall model architecture
  - **MOE.png:** Mixture of Experts implementation

---

## Usage

 1. Encode MIDI Files
```bash
python encode_all.py --midi_dirs path/to/midi/files --text_dir output/text/dir --num_processes 8
```
2. Clean Initial Silences
```bash
python first_silence_cleaner.py --dirs output/text/dir
```
3. Generate Tokenizer
```bash
python simple_tokenizer.py --dirs output/text/dir --output midi_tokenizer.json
```
4. Augment Encoded Data
```bash
python augment_all.py --input_dirs output/text/dir --output_dir augmented/text/dir --num_processes 8
```
5. Prepare Training Data
```bash
python prepare.py --main_data_path output/text/dir --augmented_data_path augmented/text/dir --output_base_dir processed/data --tokenizer_paths tokenizers/midi_tokenizer.json --augmented_percentage 30.0
```
6. Train Model
```bash
python train.py
```
7. Generate Music
```bash
python sample.py --out_dir models/my_model --num_samples 5 --temperature 0.9 --top_k 100
```
### Configuration
Customize the system via config.py.

Model Architecture
```
n_layer = 3
n_head = 12
n_embd = 768
dropout = 0.1
```
MoE Parameters
```
n_shared_experts = 2
n_routed_experts = 2
top_k_experts = 1
bias_update_speed = 0.01
balance_factor = 0.01
```
Training Parameters
```
batch_size = 32
block_size = 512
learning_rate = 3e-4
max_iters = 150000
weight_decay = 0.03
```
### Adaptability
While specifically developed for MIDI music generation, this codebase can be adapted for other sequential data types (e.g., natural language) by modifying:

1. Tokenizer: ``simple_tokenizer.py``
2. Encoding/Decoding Logic: ``encoding_decoding.py``
3. Data Preparation Pipeline: ``prepare.py``
4. Augmentation Strategies: ``augment_all.py``

The core architecture (MLA and MoE) remains domain-agnostic.

### Requirements
```
Python: 3.8+
PyTorch: 2.0+
Music21
TensorBoard
tqdm
numpy
psutil
```
### Credits
* nanoGPT by Andrej Karpathy
Foundation of this implementation

* DeepSeek-V3
Inspiration for MLA and MoE architectures

This project merges these pioneering works with specialized modifications for music generation.
